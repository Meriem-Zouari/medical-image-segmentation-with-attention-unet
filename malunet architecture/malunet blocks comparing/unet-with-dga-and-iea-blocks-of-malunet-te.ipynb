{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13070736,"sourceType":"datasetVersion","datasetId":8277861}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nimport time\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport os\nimport glob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nimport glob\n\nBASE_PATH = \"/kaggle/input/dataset/new_data\"\n\n# Define folders for train, validation, and test\nTRAIN_IMAGES_DIR = os.path.join(BASE_PATH, \"train\", \"images\")\nTRAIN_MASKS_DIR = os.path.join(BASE_PATH, \"train\", \"GT_TE\")\n\nVAL_IMAGES_DIR = os.path.join(BASE_PATH, \"valid\", \"images\")\nVAL_MASKS_DIR = os.path.join(BASE_PATH, \"valid\", \"GT_TE\")\n\nTEST_IMAGES_DIR = os.path.join(BASE_PATH, \"test\", \"images\")\nTEST_MASKS_DIR = os.path.join(BASE_PATH, \"test\", \"GT_TE\")\n\n# Check if files are loaded correctly\nprint(\"Train images:\", len(os.listdir(TRAIN_IMAGES_DIR)))\nprint(\"Train masks:\", len(os.listdir(TRAIN_MASKS_DIR)))\nprint(\"Validation images:\", len(os.listdir(VAL_IMAGES_DIR)))\nprint(\"Validation masks:\", len(os.listdir(VAL_MASKS_DIR)))\nprint(\"Test images:\", len(os.listdir(TEST_IMAGES_DIR)))\nprint(\"Test masks:\", len(os.listdir(TEST_MASKS_DIR)))\n\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\n\ndef load_image(image_path, mask_path):\n    \"\"\"Load and preprocess a single image-mask pair\"\"\"\n    # Load image\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_bmp(image, channels=3)  # decode as RGB\n    image = tf.image.rgb_to_grayscale(image)        # convert to grayscale\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n    image = tf.cast(image, tf.float32) / 255.0\n\n    # Load mask\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_bmp(mask, channels=3)\n    mask = tf.image.rgb_to_grayscale(mask)\n    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])\n    mask = tf.cast(mask, tf.float32) / 255.0\n    mask = tf.where(mask > 0.5, 1.0, 0.0)  # binarize\n\n    return image, mask\n\n\n\ndef create_dataset(image_dir, mask_dir, batch_size=8, shuffle=True):\n    \"\"\"Create tf.data dataset for training, validation, or testing\"\"\"\n    image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.bmp\")))\n    mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.bmp\")))\n\n    print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks in {image_dir}\")\n\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=100)\n\n    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n\n\nBATCH_SIZE = 16\ntrain_dataset = create_dataset(TRAIN_IMAGES_DIR, TRAIN_MASKS_DIR, batch_size=BATCH_SIZE)\nval_dataset = create_dataset(VAL_IMAGES_DIR, VAL_MASKS_DIR, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataset = create_dataset(TEST_IMAGES_DIR, TEST_MASKS_DIR, batch_size=1, shuffle=False)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# ---------------- Custom GroupNorm ----------------\nclass GroupNormalization(layers.Layer):\n    def __init__(self, groups=4, axis=-1, epsilon=1e-5, **kwargs):\n        super(GroupNormalization, self).__init__(**kwargs)\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\"Axis {} of input tensor should have a defined dimension\".format(self.axis))\n        if dim % self.groups != 0:\n            raise ValueError(\"Number of channels ({}) must be divisible by groups ({})\".format(dim, self.groups))\n        self.gamma = self.add_weight(shape=(dim,),\n                                     initializer=\"ones\",\n                                     trainable=True,\n                                     name=\"gamma\")\n        self.beta = self.add_weight(shape=(dim,),\n                                    initializer=\"zeros\",\n                                    trainable=True,\n                                    name=\"beta\")\n        super(GroupNormalization, self).build(input_shape)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        N, H, W, C = input_shape[0], input_shape[1], input_shape[2], input_shape[3]\n\n        G = self.groups\n        x = tf.reshape(inputs, [N, H, W, G, C // G])\n\n        mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n        x = (x - mean) / tf.sqrt(var + self.epsilon)\n\n        x = tf.reshape(x, [N, H, W, C])\n        return x * self.gamma + self.beta\n\n# ---------------- DepthWiseConv2D ----------------\ndef DepthWiseConv2D(filters, kernel_size=3, dilation_rate=1, padding=\"same\", activation=None):\n    return models.Sequential([\n        layers.DepthwiseConv2D(kernel_size, padding=padding, dilation_rate=dilation_rate),\n        GroupNormalization(groups=4),\n        layers.Conv2D(filters, 1, padding=\"same\"),\n        layers.Activation(activation) if activation else layers.Activation(\"linear\")\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- Gated Attention Unit ----------------\nclass GatedAttentionUnit(layers.Layer):\n    def __init__(self, in_c, out_c, kernel_size=3):\n        super(GatedAttentionUnit, self).__init__()\n        self.w1 = models.Sequential([\n            DepthWiseConv2D(in_c, kernel_size, activation=\"linear\"),\n            layers.Activation(\"sigmoid\")\n        ])\n        self.w2 = models.Sequential([\n            DepthWiseConv2D(in_c, kernel_size + 2, activation=\"gelu\"),\n        ])\n        self.wo = models.Sequential([\n            DepthWiseConv2D(out_c, kernel_size, activation=\"gelu\"),\n        ])\n        self.cw = layers.Conv2D(out_c, 1, padding=\"same\")\n\n    def call(self, x):\n        x1, x2 = self.w1(x), self.w2(x)\n        out = self.wo(layers.Multiply()([x1, x2]))\n        out = layers.Add()([out, self.cw(x)])\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DilatedGatedAttention(layers.Layer):\n    def __init__(self, in_c, out_c, k_size=3, dilated_ratio=[1, 2, 4], **kwargs):\n        super(DilatedGatedAttention, self).__init__(**kwargs)  # âœ… allows name, trainable, etc.\n        self.in_c = in_c\n        self.out_c = out_c\n        self.k_size = k_size\n        self.dilated_ratio = dilated_ratio\n\n        # Dilated convolutions\n        self.convs = [\n            layers.Conv2D(\n                in_c, k_size, padding=\"same\", dilation_rate=d,\n                activation=\"relu\"\n            ) for d in dilated_ratio\n        ]\n\n        # Normalization\n        self.norm = layers.LayerNormalization(axis=-1)  \n        self.conv = layers.Conv2D(in_c, 1, padding=\"same\")\n        self.gau = GatedAttentionUnit(in_c, out_c, kernel_size=k_size)\n\n    def call(self, x):\n        dilated_features = [conv(x) for conv in self.convs]\n        fused = layers.Concatenate()(dilated_features)\n        fused = self.norm(fused)\n        fused = self.conv(fused)\n        out = self.gau(fused)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ IEA Block ------------------\nclass EAblock(layers.Layer):\n    def __init__(self, channels, name=None):\n        super(EAblock, self).__init__(name=name)\n        self.channels = channels\n        self.expand_channels = channels * 4\n\n        # 1x1 Conv before attention\n        self.conv1 = layers.Conv2D(channels, kernel_size=1, padding='same', activation=None)\n        # Memory unit 1: expand\n        self.linear_0 = layers.Conv1D(self.expand_channels, kernel_size=1, use_bias=False)\n        # Memory unit 2: compress\n        self.linear_1 = layers.Conv1D(channels, kernel_size=1, use_bias=False)\n        # 1x1 Conv after attention\n        self.conv2 = layers.Conv2D(channels, kernel_size=1, padding='same', activation=None)\n        # Normalization\n        self.norm = layers.LayerNormalization(axis=-1)\n        # Activation\n        self.act = layers.Activation('gelu')\n\n    def call(self, x):\n        # Save residual\n        identity = x\n\n        # 1x1 conv\n        x = self.conv1(x)\n\n        # Flatten spatial dimensions: B, H*W, C\n        b, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n        x_flat = tf.reshape(x, [b, h*w, c])  # shape: (B, HW, C)\n\n        # Memory Unit 1: expand channels\n        attn = self.linear_0(x_flat)          # (B, HW, 4C)\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = attn / (1e-9 + tf.reduce_sum(attn, axis=-1, keepdims=True))\n\n        # Memory Unit 2: compress back\n        x_flat = self.linear_1(attn)          # (B, HW, C)\n\n        # Reshape back to (B, H, W, C)\n        x = tf.reshape(x_flat, [b, h, w, c])\n\n        # 1x1 conv + normalization\n        x = self.conv2(x)\n        x = self.norm(x)\n\n        # Add residual + activation\n        x = x + identity\n        x = self.act(x)\n\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ------------------ UNet with EA + DGA in Encoder, DGA + EA in Decoder ------------------\ndef unet_with_ea_dga(input_shape=(256,256,1)):\n    inputs = layers.Input(input_shape, name=\"input_layer\")\n\n    # ----- Encoder -----\n    c1 = layers.Conv2D(64, 3, activation='relu', padding='same', name=\"enc_c1a\")(inputs)\n    c1 = layers.Conv2D(64, 3, activation='relu', padding='same', name=\"enc_c1b\")(c1)\n    p1 = layers.MaxPooling2D((2, 2), name=\"pool1\")(c1)\n\n    c2 = layers.Conv2D(128, 3, activation='relu', padding='same', name=\"enc_c2a\")(p1)\n    c2 = layers.Conv2D(128, 3, activation='relu', padding='same', name=\"enc_c2b\")(c2)\n    p2 = layers.MaxPooling2D((2, 2), name=\"pool2\")(c2)\n\n    c3 = layers.Conv2D(256, 3, activation='relu', padding='same', name=\"enc_c3a\")(p2)\n    c3 = layers.Conv2D(256, 3, activation='relu', padding='same', name=\"enc_c3b\")(c3)\n    p3 = layers.MaxPooling2D((2, 2), name=\"pool3\")(c3)\n\n    # Encoder 4-6: EAblock -> DGA\n    c4 = layers.Conv2D(512, 3, activation='relu', padding='same', name=\"enc_c4a\")(p3)\n    c4 = layers.Conv2D(512, 3, activation='relu', padding='same', name=\"enc_c4b\")(c4)\n    c4 = EAblock(512, name=\"e_ablock_enc4\")(c4)\n    c4 = DilatedGatedAttention(512, 512, name=\"dga_block_enc4\")(c4)\n    p4 = layers.MaxPooling2D((2, 2), name=\"pool4\")(c4)\n\n    c5 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"enc_c5a\")(p4)\n    c5 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"enc_c5b\")(c5)\n    c5 = EAblock(1024, name=\"e_ablock_enc5\")(c5)\n    c5 = DilatedGatedAttention(1024, 1024, name=\"dga_block_enc5\")(c5)\n    p5 = layers.MaxPooling2D((2, 2), name=\"pool5\")(c5)\n\n    c6 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"enc_c6a\")(p5)\n    c6 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"enc_c6b\")(c6)\n    c6 = EAblock(1024, name=\"e_ablock_enc6\")(c6)\n    c6 = DilatedGatedAttention(1024, 1024, name=\"dga_block_enc6\")(c6)\n\n    # ----- Decoder 1-3: DGA -> EA -----\n    u7 = layers.Conv2DTranspose(1024, 2, strides=(2,2), padding='same', name=\"up7\")(c6)\n    u7 = layers.concatenate([u7, c5], axis=3, name=\"concat7\")\n    c7 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"dec_c7a\")(u7)\n    c7 = layers.Conv2D(1024, 3, activation='relu', padding='same', name=\"dec_c7b\")(c7)\n    c7 = DilatedGatedAttention(1024, 1024, name=\"dga_block_dec7\")(c7)\n    c7 = EAblock(1024, name=\"e_ablock_dec7\")(c7)\n\n    u8 = layers.Conv2DTranspose(512, 2, strides=(2,2), padding='same', name=\"up8\")(c7)\n    u8 = layers.concatenate([u8, c4], axis=3, name=\"concat8\")\n    c8 = layers.Conv2D(512, 3, activation='relu', padding='same', name=\"dec_c8a\")(u8)\n    c8 = layers.Conv2D(512, 3, activation='relu', padding='same', name=\"dec_c8b\")(c8)\n    c8 = DilatedGatedAttention(512, 512, name=\"dga_block_dec8\")(c8)\n    c8 = EAblock(512, name=\"e_ablock_dec8\")(c8)\n\n    u9 = layers.Conv2DTranspose(256, 2, strides=(2,2), padding='same', name=\"up9\")(c8)\n    u9 = layers.concatenate([u9, c3], axis=3, name=\"concat9\")\n    c9 = layers.Conv2D(256, 3, activation='relu', padding='same', name=\"dec_c9a\")(u9)\n    c9 = layers.Conv2D(256, 3, activation='relu', padding='same', name=\"dec_c9b\")(c9)\n    c9 = DilatedGatedAttention(256, 256, name=\"dga_block_dec9\")(c9)\n    c9 = EAblock(256, name=\"e_ablock_dec9\")(c9)\n\n    # ----- Decoder 4-5: plain convs -----\n    u10 = layers.Conv2DTranspose(128, 2, strides=(2,2), padding='same', name=\"up10\")(c9)\n    u10 = layers.concatenate([u10, c2], axis=3, name=\"concat10\")\n    c10 = layers.Conv2D(128, 3, activation='relu', padding='same', name=\"dec_c10a\")(u10)\n    c10 = layers.Conv2D(128, 3, activation='relu', padding='same', name=\"dec_c10b\")(c10)\n\n    u11 = layers.Conv2DTranspose(64, 2, strides=(2,2), padding='same', name=\"up11\")(c10)\n    u11 = layers.concatenate([u11, c1], axis=3, name=\"concat11\")\n    c11 = layers.Conv2D(64, 3, activation='relu', padding='same', name=\"dec_c11a\")(u11)\n    c11 = layers.Conv2D(64, 3, activation='relu', padding='same', name=\"dec_c11b\")(c11)\n\n    outputs = layers.Conv2D(1, 1, activation='sigmoid', name=\"output_layer\")(c11)\n\n    model = models.Model(inputs=inputs, outputs=outputs, name=\"UNet_with_EA_DGA\")\n    return model\n\n# ------------------ Test ------------------\nmodel = unet_with_ea_dga(input_shape=(256,256,1))\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coefficient(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef iou_metric(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef bce_dice_loss(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n    dice_loss = 1 - dice_coefficient(y_true, y_pred)\n    return bce + dice_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Image loader\n# -------------------------\ndef load_only_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_bmp(img, channels=3)   # decode as RGB\n    img = tf.image.resize(img, [256, 256])\n    img = tf.image.rgb_to_grayscale(img)         # convert to 1 channel\n    img = tf.cast(img, tf.float32) / 255.0\n    return img\n\ndef load_image(image_path, mask_path):\n    img = load_only_image(image_path)\n    mask = load_only_image(mask_path)\n    return img, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Attention map visualizer callback\n# -------------------------\nclass AttentionMapLogger(Callback):\n    def __init__(self, sample_image, every_n_epochs=5, save_dir=\"attention_maps\"):\n        super().__init__()\n        self.sample_image = tf.expand_dims(sample_image, axis=0)  # add batch dim\n        self.every_n_epochs = every_n_epochs\n        self.save_dir = save_dir\n        os.makedirs(save_dir, exist_ok=True)\n\n    def save_attention_maps(self, feature_maps, stage_name, epoch):\n        # feature_maps: tensor of shape (1, H, W, C)\n        fmap = feature_maps[0]  # remove batch\n        fmap_mean = tf.reduce_mean(fmap, axis=-1)  # collapse channels -> (H,W)\n        fmap_min = tf.reduce_min(fmap_mean)\n        fmap_max = tf.reduce_max(fmap_mean)\n        fmap_norm = (fmap_mean - fmap_min) / (fmap_max - fmap_min + 1e-6)\n\n        plt.figure(figsize=(5,5))\n        plt.imshow(fmap_norm, cmap='jet')\n        plt.axis('off')\n        plt.title(f\"{stage_name} Epoch {epoch}\")\n        plt.savefig(os.path.join(self.save_dir, f\"{stage_name}_epoch{epoch}.png\"))\n        plt.close()\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.every_n_epochs != 0:\n            return\n\n        # Run the model on the sample image and fetch attention maps\n        # Assumes your model returns a list of attention feature maps as second output\n        # Or that you have a model subclass which exposes encoder/decoder feature maps\n        attention_features = self.model.get_attention_features(self.sample_image)  # custom function\n\n        for i, fmap in enumerate(attention_features):\n            stage_name = f\"stage_{i+1}\"\n            self.save_attention_maps(fmap, stage_name, epoch + 1)\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport glob\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Training loop with K-Fold\n# -------------------------\nEPOCHS = 300\nBATCH_SIZE = 8\n\nearly_stopping = EarlyStopping(\n    monitor='val_dice_coefficient',\n    patience=10,\n    mode='max',\n    restore_best_weights=True\n)\n\ncheckpoint = ModelCheckpoint(\n    \"best_model.h5\",\n    monitor='val_dice_coefficient',\n    save_best_only=True,\n    mode='max'\n)\n\nimage_paths = np.array(sorted(glob.glob(os.path.join(TRAIN_IMAGES_DIR, \"*.bmp\"))))\nmask_paths  = np.array(sorted(glob.glob(os.path.join(TRAIN_MASKS_DIR, \"*.bmp\"))))\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfold_num = 1\n\nfor train_idx, val_idx in kfold.split(image_paths):\n    print(f\"\\n--- Fold {fold_num} ---\")\n\n    train_images, train_masks = image_paths[train_idx], mask_paths[train_idx]\n    val_images, val_masks = image_paths[val_idx], mask_paths[val_idx]\n\n    # -------------------------\n    # Dataset creation\n    # -------------------------\n    def dataset_from_paths(images, masks, shuffle=True):\n        dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n        if shuffle:\n            dataset = dataset.shuffle(100)\n        return dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n    train_dataset = dataset_from_paths(train_images, train_masks)\n    val_dataset = dataset_from_paths(val_images, val_masks, shuffle=False)\n\n    # -------------------------\n    # Build model\n    # -------------------------\n    model = unet_with_ea_dga(input_shape=(256,256,1))\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=bce_dice_loss,\n        metrics=['accuracy', dice_coefficient, iou_metric]\n    )\n\n    # -------------------------\n    # Feature extractor for attention visualization\n    # -------------------------\n\n    layer_names = [\n        'enc_c1b',\n        'enc_c2b',\n        'enc_c3b',\n        'enc_c4b',\n        'e_ablock_enc4',\n        'dga_block_enc4',\n        'enc_c5b',\n        'e_ablock_enc5',\n        'dga_block_enc5',\n        'enc_c6b',\n        'e_ablock_enc6',\n        'dga_block_enc6',\n        'dec_c7b',\n        'dga_block_dec7',\n        'e_ablock_dec7',\n        'dec_c8b',\n        'dga_block_dec8',\n        'e_ablock_dec8',\n        'dec_c9b',\n        'dga_block_dec9',\n        'e_ablock_dec9',\n        'dec_c10b',\n        'dec_c11b'\n    ]\n\n\n    intermediate_outputs = [model.get_layer(name).output for name in layer_names]\n    feature_model = tf.keras.Model(inputs=model.input, outputs=intermediate_outputs)\n\n    # Sample image for attention visualization\n    sample_image = load_only_image(val_images[0])\n    sample_image = tf.expand_dims(sample_image, axis=0)  # Add batch dimension\n\n    # -------------------------\n    # Custom callback for attention maps\n    # -------------------------\n    class AttentionMapLogger(tf.keras.callbacks.Callback):\n        def __init__(self, sample_image, every_n_epochs=5):\n            super().__init__()\n            self.sample_image = sample_image\n            self.every_n_epochs = every_n_epochs\n\n        def on_epoch_end(self, epoch, logs=None):\n            if (epoch + 1) % self.every_n_epochs == 0:\n                attention_features = feature_model(self.sample_image, training=False)\n                for i, fmap in enumerate(attention_features):\n                    fmap_mean = tf.reduce_mean(fmap, axis=-1)  # average over channels\n                    fmap_img = tf.squeeze(fmap_mean).numpy()\n                    plt.figure(figsize=(4,4))\n                    plt.imshow(fmap_img, cmap='jet')\n                    plt.title(f\"Epoch {epoch+1} - Layer {layer_names[i]}\")\n                    plt.axis('off')\n                    plt.show()\n\n    attention_logger = AttentionMapLogger(sample_image, every_n_epochs=5)\n\n    # -------------------------\n    # Train\n    # -------------------------\n    start_time = time.time()\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=EPOCHS,\n        callbacks=[early_stopping, checkpoint, attention_logger]\n    )\n    end_time = time.time()\n\n    # -------------------------\n    # Model summary, GFLOPs, training time\n    # -------------------------\n    model.summary()\n    try:\n        from tensorflow.python.profiler import model_analyzer\n        from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n        profile = model_analyzer.profile(\n            model, options=ProfileOptionBuilder.float_operation()\n        )\n        print(f\"GFLOPs: {profile.total_float_ops / 1e9}\")\n    except:\n        print(\"GFLOPs calculation not available, use tensorflow profiler for exact computation.\")\n\n    print(f\"Training time for fold {fold_num}: {end_time - start_time:.2f}s\")\n    fold_num += 1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Load best model\n# -------------------------\nmodel = unet_with_ea_dga(input_shape=(256, 256, 1))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n              loss=bce_dice_loss,\n              metrics=['accuracy', dice_coefficient, iou_metric])\n\nmodel.load_weights(\"best_model.h5\")\n\n\n# -------------------------\n# Evaluate on test set\n# -------------------------\nresults = model.evaluate(test_dataset)\nprint(\"\\nTest Results:\")\nfor name, value in zip(model.metrics_names, results):\n    print(f\"{name}: {value:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_predictions(model, dataset, num_samples=5):\n    for batch_idx, (images, masks) in enumerate(dataset.take(num_samples)):\n        preds = model.predict(images)\n        preds = (preds > 0.5).astype(\"float32\")  # Binarize predictions\n\n        for i in range(len(images)):\n            plt.figure(figsize=(12, 4))\n\n            plt.subplot(1, 3, 1)\n            plt.title(\"Input Image\")\n            plt.imshow(images[i].numpy().squeeze(), cmap=\"gray\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.title(\"Ground Truth\")\n            plt.imshow(masks[i].numpy().squeeze(), cmap=\"gray\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.title(\"Prediction\")\n            plt.imshow(preds[i].squeeze(), cmap=\"gray\")\n            plt.axis(\"off\")\n\n            plt.show()\n\n\ndisplay_predictions(model, test_dataset, num_samples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}